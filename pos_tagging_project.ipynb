{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Plp8vXtnj2_"
   },
   "outputs": [],
   "source": [
    "import pyconll\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PyAA5BK8fWQ"
   },
   "outputs": [],
   "source": [
    "def load_word2vecModel(word2vec_file):\n",
    "    \"\"\"\n",
    "    Load a pretrained word2vec model and return it as variable.\n",
    "    \n",
    "    Args:\n",
    "        word2vec_file: text filename (with full path) containing the word2vec model\n",
    "   \n",
    "    Returns:\n",
    "        w2v_model: variable containing the loaded model    \n",
    "    \"\"\"\n",
    "    print(\"Loading word2vec model...\")\n",
    "    import os\n",
    "\n",
    "    w2v_model = {}\n",
    "    with open(word2vec_file) as f:\n",
    "        for line in f:\n",
    "            word, wordVector = line.split(maxsplit=1)\n",
    "            wordVector = np.fromstring(wordVector, 'f', sep=' ')\n",
    "            w2v_model[word] = wordVector\n",
    "    \n",
    "    print(\"Done.\")        \n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RV3jO3-EPEMO",
    "outputId": "0a9994e6-254a-4351-e8e9-67d74ddd7bde"
   },
   "outputs": [],
   "source": [
    "def generate_extra_embedding_vecs(EMBEDDING_DIM,seed_state=42):\n",
    "    \"\"\"\n",
    "    Generate random embedding vectors for the tags 'EOS' (end of sentence),\n",
    "    'PAD' (for zero-padding) and 'OOV' (out of vocabulary).\n",
    "    \n",
    "    Args:\n",
    "        EMBEDDING_DIM (int)\n",
    "        (seed_state) (int): for the random generator, to have predictable values [default: 42]\n",
    "        \n",
    "    Returns:\n",
    "        rand_embed_vecs (list of 3 elements, each one a random vector for the tags: [oov, eos, pad])\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed_state)\n",
    "    oov_vec = np.random.normal(size=EMBEDDING_DIM)\n",
    "    #oov_vec.shape\n",
    "\n",
    "    eos_vec = np.random.normal(size=EMBEDDING_DIM)\n",
    "    #eos_vec.shape\n",
    "\n",
    "    pad_vec = np.random.normal(size=EMBEDDING_DIM)\n",
    "    #pad_vec.shape\n",
    "    \n",
    "    return [oov_vec, eos_vec, pad_vec]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TooRupjzh708"
   },
   "outputs": [],
   "source": [
    "def tag_encoding_dictionary(conllu_file):\n",
    "    \"\"\"\n",
    "    Create dictionary for encoding all the unique tags found in the conllu-formatted file into integers\n",
    "\n",
    "    Args:\n",
    "        conllu_file: filename (with whole path) of conllu-formatted file that we use\n",
    "\n",
    "    Returns:\n",
    "        tag_dict: tag dictionary used for encoding\n",
    "    \"\"\"  \n",
    "\n",
    "      #get unique list of tags from file, bash (didn't manage from parser Pyconll),\n",
    "    import subprocess\n",
    "\n",
    "    tag_list = subprocess.check_output(\"awk '{ print $4 }' \" +  conllu_file + \" | sort | uniq\", shell=True)\n",
    "    tag_list = tag_list.decode().splitlines()\n",
    "    tag_list[:] = [x for x in tag_list if x] #remove empty strings\n",
    "\n",
    "    tag_dict = {}\n",
    "    for int_code, tag in enumerate(tag_list):\n",
    "        tag_dict[tag] = int_code\n",
    "\n",
    "        tag_dict['EOS'] = int_code+1\n",
    "        tag_dict['PAD'] = int_code+2\n",
    "\n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z0K7cEBe1ueI",
    "outputId": "ca254ef2-92cc-4667-cc9a-7438ec918571"
   },
   "outputs": [],
   "source": [
    "def word2vec_data_encoding(data,word2vec_model,MAX_SEQUENCE_LEN, EMBEDDING_DIM, extra_embeddings):\n",
    "    \"\"\"\n",
    "    Encode input data into: 1) arrays of word2vec embeddings and 2) corresponding labels ('tags')\n",
    "    \n",
    "    Args:\n",
    "        data (PyConll object, with N sentences): the data to be encoded\n",
    "        (https://pyconll.readthedocs.io/en/stable/index.html)\n",
    "        word2vec_model (dictionary, keys=words, values: embeddings): pretrained word2vec model used in encoding\n",
    "        MAX_SEQUENNCE_LEN (int): maximum length of sentence (of training data)\n",
    "        EMBEDDING_DIM (int): length of the embedding vectors of word2vec_model\n",
    "        extra_embeddings (list of 3 embedding vectors): this list corresponds to embeddings of \n",
    "       [OOV, EOS, PAD], respectively.\n",
    "        \n",
    "    Returns:\n",
    "        sentences_X (numpy array of size (N x MAX_SEQUENCE_LEN x EMBEDDING_DIM): input data for the classifier\n",
    "        tags_y (numpy array of size (N x MAX_SEQUENCE_LEN): output data (labels) for the classifier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        len(word2vec_model['the']) == EMBEDDING_DIM\n",
    "    except:\n",
    "        raise Exception('Error: mismatch between EMBEDDING_DIM and dimension of word2vec_model vectors')\n",
    "    print(\"Encoding data into embeddings...\")\n",
    "    \n",
    "    oov_vec= extra_embeddings[0]\n",
    "    eos_vec= extra_embeddings[1]\n",
    "    pad_vec= extra_embeddings[2]\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    sentences_X = np.empty((N, MAX_SEQUENCE_LEN,EMBEDDING_DIM))\n",
    "    tags_y = np.empty((N, MAX_SEQUENCE_LEN))\n",
    "\n",
    "    for idx_sentence,sentence in enumerate(data):\n",
    "        #print(sentence)\n",
    "\n",
    "        idx_eos = len(sentence)\n",
    "        for idx_word, token in enumerate(sentence):\n",
    "            token_fixed = token.form.lower()\n",
    "        \n",
    "            if token_fixed in word2vec_model:\n",
    "                #print('in:')\n",
    "                #print(token_fixed)\n",
    "                sentences_X[idx_sentence,idx_word,:] = word2vec_model[token_fixed]\n",
    "                tags_y[idx_sentence,idx_word] = tag_dict[token.upos]\n",
    "            else:\n",
    "                #print('OOV:')\n",
    "                #print(token_fixed)\n",
    "                sentences_X[idx_sentence,idx_word,:] = oov_vec\n",
    "                tags_y[idx_sentence,idx_word] = tag_dict[token.upos]\n",
    "\n",
    "        #print('EOS')\n",
    "        sentences_X[idx_sentence,idx_eos] = eos_vec\n",
    "        tags_y[idx_sentence,idx_eos] = tag_dict['EOS']\n",
    "\n",
    "        #add zero-padding if necessary\n",
    "        if idx_eos < MAX_SEQUENCE_LEN:\n",
    "            sentences_X[(idx_sentence,range(idx_eos+1,MAX_SEQUENCE_LEN))]= pad_vec\n",
    "            tags_y[(idx_sentence,range(idx_eos+1,MAX_SEQUENCE_LEN))] = tag_dict['PAD']\n",
    "    print(\"Done.\")\n",
    "    return [sentences_X, tags_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZUqPtqkPg6Y"
   },
   "source": [
    "## Load conllu-formatted data and pre-trained word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained model for the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "C4ZbeY2a8ifd",
    "outputId": "01344ffb-b369-48a4-98ba-29eeac4a03dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "word2vec_file = 'data/glove.6B/glove.6B.100d.txt'\n",
    "word2vec_model = load_word2vecModel(word2vec_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkAIU7LyMiC9"
   },
   "outputs": [],
   "source": [
    " EMBEDDING_DIM = len(word2vec_model['the']) # 100-dimensional word embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data (train, validation and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMbCrS46wydz"
   },
   "outputs": [],
   "source": [
    "conllu_train_file = 'data/ud-1.2/en/en-ud-train.conllu'\n",
    "conllu_val_file = 'data/ud-1.2/en/en-ud-dev.conllu'\n",
    "conllu_test_file = 'data/ud-1.2/en/en-ud-test.conllu'\n",
    "\n",
    "data_train = pyconll.load_from_file(conllu_train_file)\n",
    "data_val = pyconll.load_from_file(conllu_val_file)\n",
    "data_test = pyconll.load_from_file(conllu_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LEN = len(max(data_train, key=len)) +1 #since we add an EOS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to encode tags into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IS_lpu5XYvY"
   },
   "outputs": [],
   "source": [
    "tag_dict = tag_encoding_dictionary(conllu_train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random vectors as embeddings for tags 'EOS', 'PAD', and 'OOV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list with elements: [oov_vec, eos_vec, pad_vec]\n",
    "extra_embeddings = generate_extra_embedding_vecs(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode conllu-formatted data into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding data into embeddings...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "[X_train, y_train] = word2vec_data_encoding(data_train,word2vec_model,MAX_SEQUENCE_LEN, EMBEDDING_DIM, extra_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding data into embeddings...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "[X_val, y_val] = word2vec_data_encoding(data_val,word2vec_model,MAX_SEQUENCE_LEN, EMBEDDING_DIM, extra_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding data into embeddings...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "[X_test, y_test] = word2vec_data_encoding(data_test,word2vec_model,MAX_SEQUENCE_LEN, EMBEDDING_DIM, extra_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVTmBbI6PLeY"
   },
   "source": [
    "## POS-tag model (feat extraction (encoder) + classification (decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dIdNm2RKPJhL",
    "outputId": "b186cde4-5a5f-4b80-f4d2-ae40fcd09da8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import L1L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "RvG-NCY_OAgS",
    "outputId": "43efa6dc-cd14-490e-96dc-eb619b4b9745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 160, 50)           30200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 160, 19)           969       \n",
      "=================================================================\n",
      "Total params: 31,169\n",
      "Trainable params: 31,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "hidden_units = 50\n",
    "num_tags = len(tag_dict)\n",
    "model.add(InputLayer(input_shape=(MAX_SEQUENCE_LEN, EMBEDDING_DIM)))\n",
    "model.add(LSTM(hidden_units, return_sequences=True))\n",
    "model.add(Dense(num_tags, activation='softmax')) # Dense can handle 3D input too\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dIdNm2RKPJhL",
    "outputId": "b186cde4-5a5f-4b80-f4d2-ae40fcd09da8"
   },
   "outputs": [],
   "source": [
    "#training settings\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "N9aD-qBp2eap",
    "outputId": "5d25d293-654f-4939-b6c0-3b59664ebc47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/79 [==============================] - 11s 141ms/step - loss: 0.6250 - accuracy: 0.8736 - val_loss: 0.3795 - val_accuracy: 0.8925\n",
      "Epoch 2/20\n",
      "79/79 [==============================] - 11s 136ms/step - loss: 0.3515 - accuracy: 0.8960 - val_loss: 0.3488 - val_accuracy: 0.8934\n",
      "Epoch 3/20\n",
      "79/79 [==============================] - 11s 145ms/step - loss: 0.3330 - accuracy: 0.8974 - val_loss: 0.3374 - val_accuracy: 0.8960\n",
      "Epoch 4/20\n",
      "79/79 [==============================] - 11s 144ms/step - loss: 0.3242 - accuracy: 0.8994 - val_loss: 0.3306 - val_accuracy: 0.8987\n",
      "Epoch 5/20\n",
      "79/79 [==============================] - 11s 136ms/step - loss: 0.3183 - accuracy: 0.9016 - val_loss: 0.3256 - val_accuracy: 0.9011\n",
      "Epoch 6/20\n",
      "79/79 [==============================] - 11s 136ms/step - loss: 0.3138 - accuracy: 0.9034 - val_loss: 0.3217 - val_accuracy: 0.9028\n",
      "Epoch 7/20\n",
      "79/79 [==============================] - 11s 136ms/step - loss: 0.3103 - accuracy: 0.9046 - val_loss: 0.3185 - val_accuracy: 0.9032\n",
      "Epoch 8/20\n",
      "79/79 [==============================] - 11s 136ms/step - loss: 0.3073 - accuracy: 0.9056 - val_loss: 0.3157 - val_accuracy: 0.9040\n",
      "Epoch 9/20\n",
      "79/79 [==============================] - 11s 137ms/step - loss: 0.3047 - accuracy: 0.9063 - val_loss: 0.3134 - val_accuracy: 0.9044\n",
      "Epoch 10/20\n",
      "79/79 [==============================] - 11s 137ms/step - loss: 0.3024 - accuracy: 0.9069 - val_loss: 0.3113 - val_accuracy: 0.9049\n",
      "Epoch 11/20\n",
      "79/79 [==============================] - 11s 138ms/step - loss: 0.3004 - accuracy: 0.9072 - val_loss: 0.3094 - val_accuracy: 0.9052\n",
      "Epoch 12/20\n",
      "79/79 [==============================] - 11s 138ms/step - loss: 0.2986 - accuracy: 0.9075 - val_loss: 0.3077 - val_accuracy: 0.9054\n",
      "Epoch 13/20\n",
      "79/79 [==============================] - 11s 139ms/step - loss: 0.2969 - accuracy: 0.9078 - val_loss: 0.3061 - val_accuracy: 0.9055\n",
      "Epoch 14/20\n",
      "79/79 [==============================] - 11s 139ms/step - loss: 0.2953 - accuracy: 0.9080 - val_loss: 0.3047 - val_accuracy: 0.9056\n",
      "Epoch 15/20\n",
      "79/79 [==============================] - 11s 139ms/step - loss: 0.2939 - accuracy: 0.9082 - val_loss: 0.3033 - val_accuracy: 0.9056\n",
      "Epoch 16/20\n",
      "79/79 [==============================] - 12s 146ms/step - loss: 0.2926 - accuracy: 0.9083 - val_loss: 0.3021 - val_accuracy: 0.9058\n",
      "Epoch 17/20\n",
      "79/79 [==============================] - 11s 142ms/step - loss: 0.2913 - accuracy: 0.9084 - val_loss: 0.3009 - val_accuracy: 0.9058\n",
      "Epoch 18/20\n",
      "79/79 [==============================] - 11s 143ms/step - loss: 0.2901 - accuracy: 0.9086 - val_loss: 0.2997 - val_accuracy: 0.9059\n",
      "Epoch 19/20\n",
      "79/79 [==============================] - 12s 146ms/step - loss: 0.2890 - accuracy: 0.9087 - val_loss: 0.2987 - val_accuracy: 0.9060\n",
      "Epoch 20/20\n",
      "79/79 [==============================] - 11s 144ms/step - loss: 0.2879 - accuracy: 0.9088 - val_loss: 0.2977 - val_accuracy: 0.9061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc4904cbdd8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "#model.fit(X_train, np_utils.to_categorical(y_train,num_tags), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, y_val))\n",
    "model.fit(X_train, np_utils.to_categorical(y_train,num_tags), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2200 - accuracy: 0.9301\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, np_utils.to_categorical(y_test,num_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set - accuracy: 93.00944805145264\n"
     ]
    }
   ],
   "source": [
    "#print(f\"{model.metrics_names[0]}: {scores[0] * 100}\") #loss\n",
    "print(\"Test set - \" f\"{model.metrics_names[1]}: {scores[1] * 100}\") #accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afClXSqDVqTF"
   },
   "outputs": [],
   "source": [
    "#TODOs\n",
    "#1. Do the training using the given validation data (now just training data is split).\n",
    "#2. Include GPU option, to train the keras model using the GPU"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pos-tag project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
